{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXXzKhTcUaAk"
      },
      "source": [
        "# importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "import re\n",
        "import random\n",
        "import string\n",
        "from keras.callbacks import EarlyStopping"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H89r-T9gWxWD"
      },
      "source": [
        "data1 = \"/content/small_vocab_en\"\n",
        "data2 = \"/content/small_vocab_fr\"\n",
        "with open(data1, 'r', encoding = 'utf-8') as f:\n",
        "  lines = f.read().strip().split('\\n')\n",
        "with open(data2, 'r', encoding = 'utf-8') as f:\n",
        "  line1 = f.read().strip().split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqLPIxVTY6Cd"
      },
      "source": [
        "# data cleaning\n",
        "lines = [\" \".join(re.findall(r\"[A-Za-z0-9]+\",line)) for line in lines]\n",
        "line1 = [\" \".join(re.findall(r\"[A-Za-z0-9]+\",line)) for line in line1]\n",
        "pairs = list(zip(lines, line1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rBDUC07bODV"
      },
      "source": [
        "import numpy as np\n",
        "input_docs = []\n",
        "target_docs = []\n",
        "input_tokens = set()\n",
        "target_tokens = set()\n",
        "for line in pairs[:500]:\n",
        "  \n",
        "  input_doc, target_doc = line[0], line[1]\n",
        "  input_docs.append(input_doc)\n",
        "  target_doc = '<START> ' +target_doc + ' <END>'\n",
        "  target_docs.append(target_doc)\n",
        "  for token in input_doc.split():\n",
        "    if token not in input_tokens:\n",
        "      input_tokens.add(token)\n",
        "  for token in target_doc.split(\" \"):\n",
        "    if token not in target_tokens:\n",
        "      target_tokens.add(token)\n",
        "input_tokens = sorted(list(input_tokens))\n",
        "target_tokens = sorted(list(target_tokens))\n",
        "num_encoder_tokens = len(input_tokens)\n",
        "num_Decoder_tokens = len(target_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USFf7VEV8yS2",
        "outputId": "469226de-a4fd-44f8-8a81-1d6776b4f1dd"
      },
      "source": [
        "# creating input and target features dictionary\n",
        "input_feature_dict = dict([(token, i) for i, token in enumerate(input_tokens)])\n",
        "target_feature_dict = dict([(token, i) for i, token in enumerate(target_tokens)])\n",
        "\n",
        "rev_input_feature_dict = dict((i, token) for token, i in input_feature_dict.items())\n",
        "rev_target_feature_dict = dict((i, token) for token, i in target_feature_dict.items())\n",
        "target_feature_dict"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<END>': 0,\n",
              " '<START>': 1,\n",
              " 'a': 2,\n",
              " 'able': 3,\n",
              " 'agr': 4,\n",
              " 'aim': 5,\n",
              " 'aimait': 6,\n",
              " 'aime': 7,\n",
              " 'aiment': 8,\n",
              " 'aimez': 9,\n",
              " 'aimons': 10,\n",
              " 'all': 11,\n",
              " 'amusant': 12,\n",
              " 'animal': 13,\n",
              " 'animaux': 14,\n",
              " 'ao': 15,\n",
              " 'au': 16,\n",
              " 'automne': 17,\n",
              " 'automobile': 18,\n",
              " 'aux': 19,\n",
              " 'avril': 20,\n",
              " 'banane': 21,\n",
              " 'bananes': 22,\n",
              " 'beau': 23,\n",
              " 'belle': 24,\n",
              " 'blanc': 25,\n",
              " 'blanche': 26,\n",
              " 'bleu': 27,\n",
              " 'bleue': 28,\n",
              " 'california': 29,\n",
              " 'californie': 30,\n",
              " 'calme': 31,\n",
              " 'camion': 32,\n",
              " 'ce': 33,\n",
              " 'cembre': 34,\n",
              " 'cet': 35,\n",
              " 'cette': 36,\n",
              " 'chat': 37,\n",
              " 'chaud': 38,\n",
              " 'chaude': 39,\n",
              " 'chaux': 40,\n",
              " 'che': 41,\n",
              " 'cher': 42,\n",
              " 'ches': 43,\n",
              " 'cheval': 44,\n",
              " 'chevaux': 45,\n",
              " 'chien': 46,\n",
              " 'chiens': 47,\n",
              " 'chine': 48,\n",
              " 'citron': 49,\n",
              " 'citrons': 50,\n",
              " 'comme': 51,\n",
              " 'conduisait': 52,\n",
              " 'conduit': 53,\n",
              " 'cours': 54,\n",
              " 'd': 55,\n",
              " 'de': 56,\n",
              " 'dernier': 57,\n",
              " 'des': 58,\n",
              " 'difficile': 59,\n",
              " 'doux': 60,\n",
              " 'e': 61,\n",
              " 'elle': 62,\n",
              " 'en': 63,\n",
              " 'enneig': 64,\n",
              " 'entre': 65,\n",
              " 'espagnol': 66,\n",
              " 'est': 67,\n",
              " 'et': 68,\n",
              " 'f': 69,\n",
              " 'fait': 70,\n",
              " 'favori': 71,\n",
              " 'fraise': 72,\n",
              " 'fraises': 73,\n",
              " 'france': 74,\n",
              " 'frisquet': 75,\n",
              " 'froid': 76,\n",
              " 'fruit': 77,\n",
              " 'fruits': 78,\n",
              " 'g': 79,\n",
              " 'gel': 80,\n",
              " 'glaciales': 81,\n",
              " 'grand': 82,\n",
              " 'grande': 83,\n",
              " 'gros': 84,\n",
              " 'habituellement': 85,\n",
              " 'hiver': 86,\n",
              " 'humide': 87,\n",
              " 'i': 88,\n",
              " 'il': 89,\n",
              " 'ils': 90,\n",
              " 'inde': 91,\n",
              " 'j': 92,\n",
              " 'jamais': 93,\n",
              " 'janvier': 94,\n",
              " 'jaune': 95,\n",
              " 'je': 96,\n",
              " 'jersey': 97,\n",
              " 'juillet': 98,\n",
              " 'juin': 99,\n",
              " 'l': 100,\n",
              " 'la': 101,\n",
              " 'le': 102,\n",
              " 'les': 103,\n",
              " 'leur': 104,\n",
              " 'leurs': 105,\n",
              " 'magnifique': 106,\n",
              " 'mai': 107,\n",
              " 'mais': 108,\n",
              " 'mangue': 109,\n",
              " 'mangues': 110,\n",
              " 'mars': 111,\n",
              " 'merveilleux': 112,\n",
              " 'moins': 113,\n",
              " 'mois': 114,\n",
              " 'mon': 115,\n",
              " 'monde': 116,\n",
              " 'n': 117,\n",
              " 'ne': 118,\n",
              " 'neige': 119,\n",
              " 'neigeux': 120,\n",
              " 'new': 121,\n",
              " 'noire': 122,\n",
              " 'nos': 123,\n",
              " 'notre': 124,\n",
              " 'nous': 125,\n",
              " 'nouveau': 126,\n",
              " 'nouvelle': 127,\n",
              " 'novembre': 128,\n",
              " 'o': 129,\n",
              " 'occup': 130,\n",
              " 'octobre': 131,\n",
              " 'oiseau': 132,\n",
              " 'orange': 133,\n",
              " 'oranges': 134,\n",
              " 'p': 135,\n",
              " 'pamplemousse': 136,\n",
              " 'pamplemousses': 137,\n",
              " 'parfois': 138,\n",
              " 'paris': 139,\n",
              " 'pas': 140,\n",
              " 'pendant': 141,\n",
              " 'pense': 142,\n",
              " 'petit': 143,\n",
              " 'petite': 144,\n",
              " 'phant': 145,\n",
              " 'phants': 146,\n",
              " 'pleut': 147,\n",
              " 'pluies': 148,\n",
              " 'plus': 149,\n",
              " 'pluvieux': 150,\n",
              " 'poire': 151,\n",
              " 'poires': 152,\n",
              " 'pomme': 153,\n",
              " 'pommes': 154,\n",
              " 'portugais': 155,\n",
              " 'pourquoi': 156,\n",
              " 'pourrait': 157,\n",
              " 'pr': 158,\n",
              " 'printemps': 159,\n",
              " 'prochain': 160,\n",
              " 'qu': 161,\n",
              " 'que': 162,\n",
              " 'r': 163,\n",
              " 'raisin': 164,\n",
              " 'raisins': 165,\n",
              " 'ralement': 166,\n",
              " 're': 167,\n",
              " 'redout': 168,\n",
              " 'relaxant': 169,\n",
              " 'rendre': 170,\n",
              " 'requin': 171,\n",
              " 'rouge': 172,\n",
              " 'rouill': 173,\n",
              " 's': 174,\n",
              " 'se': 175,\n",
              " 'sec': 176,\n",
              " 'septembre': 177,\n",
              " 'ses': 178,\n",
              " 'son': 179,\n",
              " 'sont': 180,\n",
              " 'souris': 181,\n",
              " 't': 182,\n",
              " 'taient': 183,\n",
              " 'tait': 184,\n",
              " 'tats': 185,\n",
              " 'teste': 186,\n",
              " 'testons': 187,\n",
              " 'traduction': 188,\n",
              " 'traduire': 189,\n",
              " 'tranquille': 190,\n",
              " 'trop': 191,\n",
              " 'un': 192,\n",
              " 'une': 193,\n",
              " 'unis': 194,\n",
              " 'va': 195,\n",
              " 'vert': 196,\n",
              " 'verte': 197,\n",
              " 'verts': 198,\n",
              " 'vieille': 199,\n",
              " 'vieux': 200,\n",
              " 'visiter': 201,\n",
              " 'vois': 202,\n",
              " 'voiture': 203,\n",
              " 'volant': 204,\n",
              " 'vont': 205,\n",
              " 'vos': 206,\n",
              " 'votre': 207,\n",
              " 'vous': 208,\n",
              " 'vrier': 209,\n",
              " 'vu': 210}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yLbhYapI2V8"
      },
      "source": [
        "# maximum length in input and target docs\n",
        "max_input_seq_length = max([len(input_doc) for input_doc in input_docs])\n",
        "max_target_seq_length = max([len(target_doc) for target_doc in target_docs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ9N0daSRHl0"
      },
      "source": [
        "**We need three matrices of one-hot encoder to train our seq2seq model. one for encoder input, second for decoder input and third for decoder output.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNmYeSYcUQW4"
      },
      "source": [
        "encoder_input_data = np.zeros((len(input_docs), max_input_seq_length, num_encoder_tokens), dtype = 'float32')\n",
        "decoder_input_data = np.zeros((len(target_docs), max_target_seq_length, num_Decoder_tokens), dtype = 'float32')\n",
        "decoder_output_data = np.zeros((len(target_docs), max_target_seq_length, num_Decoder_tokens), dtype = 'float32')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovwy-2lCV7g4"
      },
      "source": [
        "for line, (input_doc, target_doc) in enumerate(zip(input_docs, target_docs)):\n",
        "  for timestep,token in enumerate(input_doc.split()):\n",
        "    encoder_input_data[line, timestep, input_feature_dict[token]] = 1\n",
        "  for timestep, token in enumerate(target_doc.split()):\n",
        "    decoder_input_data[line, timestep, target_feature_dict[token]] = 1\n",
        "    if timestep > 0:\n",
        "      decoder_output_data[line, timestep-1, target_feature_dict[token]] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZVD-0Bgd8zR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlbouFfyv9jm"
      },
      "source": [
        "**TRAINING SETUP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa78rdngTiCx"
      },
      "source": [
        "from tensorflow import keras \n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, LSTM, Input\n",
        "dimensionality = 256\n",
        "\n",
        "#encoder model\n",
        "encoder_input = Input(shape = (None, num_encoder_tokens))\n",
        "lstm = LSTM(dimensionality, return_state = True)\n",
        "encoder_output, encoder_hidden_state, encoder_cell_state = lstm(encoder_input)\n",
        "encoder_states = [encoder_hidden_state, encoder_cell_state]\n",
        "\n",
        "#decoder_model\n",
        "decoder_input = Input(shape = (None, num_Decoder_tokens))\n",
        "lstm = LSTM(dimensionality, return_state = True, return_sequences = True)\n",
        "decoder_output, decoder_state_hidden, decoder_cell_state = lstm(decoder_input, initial_state = encoder_states)\n",
        "decoder_dense = Dense(num_Decoder_tokens, activation = 'softmax')\n",
        "decoder_output = decoder_dense(decoder_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnZ7LJlUW4wQ"
      },
      "source": [
        "# using earlystopping function that tracks the val_loss, stops training if there is no change towards the val_loss.\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3, verbose = 1, restore_best_weights = True)\n",
        "callbacks_list = [earlystop]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOxk11FqLV2V"
      },
      "source": [
        "**BUILDING AND TRAINING ENCODER AND DECODER MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK26__bKLOGK",
        "outputId": "a73a8c53-5ecc-417f-d8ac-667037b3915e"
      },
      "source": [
        "#model\n",
        "training_model = Model([encoder_input, decoder_input], decoder_output)\n",
        "# compiling\n",
        "training_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "# fitiing the model\n",
        "history = training_model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size = 256, epochs = 100, validation_split = 0.2)\n",
        "#saving the model\n",
        "training_model.save('training_model.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 10s 3s/step - loss: 0.6787 - accuracy: 0.2832 - val_loss: 0.6672 - val_accuracy: 0.8687\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6813 - accuracy: 0.8710 - val_loss: 0.6649 - val_accuracy: 0.8915\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6715 - accuracy: 0.8897 - val_loss: 0.6612 - val_accuracy: 0.8904\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6770 - accuracy: 0.8872 - val_loss: 0.6537 - val_accuracy: 0.8897\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6613 - accuracy: 0.8872 - val_loss: 0.6160 - val_accuracy: 0.8889\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6257 - accuracy: 0.8862 - val_loss: 0.5980 - val_accuracy: 0.8891\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6113 - accuracy: 0.8863 - val_loss: 0.5905 - val_accuracy: 0.8897\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6052 - accuracy: 0.8870 - val_loss: 0.5851 - val_accuracy: 0.8898\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5980 - accuracy: 0.8876 - val_loss: 0.5794 - val_accuracy: 0.8896\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5941 - accuracy: 0.8871 - val_loss: 0.5707 - val_accuracy: 0.8900\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5826 - accuracy: 0.8884 - val_loss: 0.5638 - val_accuracy: 0.8885\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5783 - accuracy: 0.8869 - val_loss: 0.5602 - val_accuracy: 0.8903\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5745 - accuracy: 0.8886 - val_loss: 0.5594 - val_accuracy: 0.8908\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5728 - accuracy: 0.8879 - val_loss: 0.5580 - val_accuracy: 0.8901\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5695 - accuracy: 0.8881 - val_loss: 0.5556 - val_accuracy: 0.8901\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5682 - accuracy: 0.8881 - val_loss: 0.5535 - val_accuracy: 0.8889\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5637 - accuracy: 0.8873 - val_loss: 0.5521 - val_accuracy: 0.8899\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5612 - accuracy: 0.8886 - val_loss: 0.5519 - val_accuracy: 0.8903\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5647 - accuracy: 0.8883 - val_loss: 0.5509 - val_accuracy: 0.8899\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5618 - accuracy: 0.8883 - val_loss: 0.5499 - val_accuracy: 0.8896\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5610 - accuracy: 0.8882 - val_loss: 0.5489 - val_accuracy: 0.8886\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5599 - accuracy: 0.8881 - val_loss: 0.5485 - val_accuracy: 0.8900\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5578 - accuracy: 0.8892 - val_loss: 0.5484 - val_accuracy: 0.8905\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5581 - accuracy: 0.8891 - val_loss: 0.5480 - val_accuracy: 0.8905\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5592 - accuracy: 0.8893 - val_loss: 0.5481 - val_accuracy: 0.8899\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5568 - accuracy: 0.8900 - val_loss: 0.5479 - val_accuracy: 0.8904\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5572 - accuracy: 0.8898 - val_loss: 0.5464 - val_accuracy: 0.8900\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5569 - accuracy: 0.8893 - val_loss: 0.5460 - val_accuracy: 0.8898\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5569 - accuracy: 0.8895 - val_loss: 0.5462 - val_accuracy: 0.8913\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5574 - accuracy: 0.8907 - val_loss: 0.5454 - val_accuracy: 0.8915\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5560 - accuracy: 0.8910 - val_loss: 0.5446 - val_accuracy: 0.8915\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5542 - accuracy: 0.8902 - val_loss: 0.5439 - val_accuracy: 0.8910\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5556 - accuracy: 0.8897 - val_loss: 0.5440 - val_accuracy: 0.8901\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5561 - accuracy: 0.8897 - val_loss: 0.5441 - val_accuracy: 0.8920\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5561 - accuracy: 0.8905 - val_loss: 0.5435 - val_accuracy: 0.8914\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5559 - accuracy: 0.8901 - val_loss: 0.5425 - val_accuracy: 0.8914\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5519 - accuracy: 0.8907 - val_loss: 0.5416 - val_accuracy: 0.8903\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5488 - accuracy: 0.8910 - val_loss: 0.5408 - val_accuracy: 0.8907\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5510 - accuracy: 0.8910 - val_loss: 0.5399 - val_accuracy: 0.8918\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5494 - accuracy: 0.8922 - val_loss: 0.5401 - val_accuracy: 0.8920\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5502 - accuracy: 0.8921 - val_loss: 0.5399 - val_accuracy: 0.8918\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5494 - accuracy: 0.8924 - val_loss: 0.5378 - val_accuracy: 0.8916\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5458 - accuracy: 0.8919 - val_loss: 0.5369 - val_accuracy: 0.8909\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5471 - accuracy: 0.8916 - val_loss: 0.5370 - val_accuracy: 0.8916\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5469 - accuracy: 0.8912 - val_loss: 0.5361 - val_accuracy: 0.8922\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5443 - accuracy: 0.8934 - val_loss: 0.5353 - val_accuracy: 0.8925\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5455 - accuracy: 0.8929 - val_loss: 0.5356 - val_accuracy: 0.8918\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5454 - accuracy: 0.8922 - val_loss: 0.5352 - val_accuracy: 0.8935\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5463 - accuracy: 0.8933 - val_loss: 0.5335 - val_accuracy: 0.8930\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5421 - accuracy: 0.8933 - val_loss: 0.5328 - val_accuracy: 0.8931\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5423 - accuracy: 0.8931 - val_loss: 0.5325 - val_accuracy: 0.8752\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5406 - accuracy: 0.8893 - val_loss: 0.5341 - val_accuracy: 0.8561\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5440 - accuracy: 0.8680 - val_loss: 0.5381 - val_accuracy: 0.8106\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5461 - accuracy: 0.8445 - val_loss: 0.5397 - val_accuracy: 0.7825\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5454 - accuracy: 0.8411 - val_loss: 0.5374 - val_accuracy: 0.7824\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5450 - accuracy: 0.8325 - val_loss: 0.5348 - val_accuracy: 0.7825\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5444 - accuracy: 0.8232 - val_loss: 0.5362 - val_accuracy: 0.7822\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5411 - accuracy: 0.8188 - val_loss: 0.5370 - val_accuracy: 0.8916\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5386 - accuracy: 0.8936 - val_loss: 0.5331 - val_accuracy: 0.8943\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5445 - accuracy: 0.8931 - val_loss: 0.5352 - val_accuracy: 0.8934\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5428 - accuracy: 0.8931 - val_loss: 0.5355 - val_accuracy: 0.8931\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5448 - accuracy: 0.8922 - val_loss: 0.5351 - val_accuracy: 0.8948\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5458 - accuracy: 0.8944 - val_loss: 0.5336 - val_accuracy: 0.8956\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5420 - accuracy: 0.8946 - val_loss: 0.5327 - val_accuracy: 0.8946\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5442 - accuracy: 0.8946 - val_loss: 0.5344 - val_accuracy: 0.8948\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5434 - accuracy: 0.8947 - val_loss: 0.5314 - val_accuracy: 0.8946\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5399 - accuracy: 0.8949 - val_loss: 0.5326 - val_accuracy: 0.8940\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5414 - accuracy: 0.8941 - val_loss: 0.5295 - val_accuracy: 0.8941\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5364 - accuracy: 0.8942 - val_loss: 0.5298 - val_accuracy: 0.8937\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5406 - accuracy: 0.8930 - val_loss: 0.5368 - val_accuracy: 0.8933\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5459 - accuracy: 0.8910 - val_loss: 0.5270 - val_accuracy: 0.8943\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5479 - accuracy: 0.8917 - val_loss: 0.6204 - val_accuracy: 0.8805\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6309 - accuracy: 0.8798 - val_loss: 0.6268 - val_accuracy: 0.8840\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6338 - accuracy: 0.8823 - val_loss: 0.6072 - val_accuracy: 0.8864\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6165 - accuracy: 0.8843 - val_loss: 0.6068 - val_accuracy: 0.8835\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6163 - accuracy: 0.8816 - val_loss: 0.6046 - val_accuracy: 0.8848\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6144 - accuracy: 0.8829 - val_loss: 0.6095 - val_accuracy: 0.8835\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6171 - accuracy: 0.8821 - val_loss: 0.6010 - val_accuracy: 0.8835\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6111 - accuracy: 0.8819 - val_loss: 0.5927 - val_accuracy: 0.8835\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6054 - accuracy: 0.8814 - val_loss: 0.5971 - val_accuracy: 0.8845\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6099 - accuracy: 0.8818 - val_loss: 0.6009 - val_accuracy: 0.8842\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6115 - accuracy: 0.8823 - val_loss: 0.6020 - val_accuracy: 0.8839\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6098 - accuracy: 0.8825 - val_loss: 0.6030 - val_accuracy: 0.8839\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6118 - accuracy: 0.8821 - val_loss: 0.6033 - val_accuracy: 0.8839\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6137 - accuracy: 0.8818 - val_loss: 0.6022 - val_accuracy: 0.8840\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6130 - accuracy: 0.8819 - val_loss: 0.5994 - val_accuracy: 0.8841\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6081 - accuracy: 0.8824 - val_loss: 0.5957 - val_accuracy: 0.8841\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6046 - accuracy: 0.8823 - val_loss: 0.5947 - val_accuracy: 0.8835\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6062 - accuracy: 0.8814 - val_loss: 0.5943 - val_accuracy: 0.8835\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6063 - accuracy: 0.8813 - val_loss: 0.5917 - val_accuracy: 0.8835\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6043 - accuracy: 0.8814 - val_loss: 0.5919 - val_accuracy: 0.8835\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6059 - accuracy: 0.8811 - val_loss: 0.5907 - val_accuracy: 0.8835\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6027 - accuracy: 0.8815 - val_loss: 0.5876 - val_accuracy: 0.8835\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6019 - accuracy: 0.8810 - val_loss: 0.5867 - val_accuracy: 0.8837\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.6000 - accuracy: 0.8813 - val_loss: 0.5856 - val_accuracy: 0.8841\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5935 - accuracy: 0.8831 - val_loss: 0.5849 - val_accuracy: 0.8849\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5947 - accuracy: 0.8832 - val_loss: 0.5845 - val_accuracy: 0.8853\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5951 - accuracy: 0.8834 - val_loss: 0.5827 - val_accuracy: 0.8858\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5929 - accuracy: 0.8836 - val_loss: 0.5821 - val_accuracy: 0.8853\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 5s 2s/step - loss: 0.5930 - accuracy: 0.8833 - val_loss: 0.5817 - val_accuracy: 0.8853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2fdrXPrfJB-"
      },
      "source": [
        "#training_model = load_model('/content/drive/My Drive/MachineTranslation/training_model.h5')\n",
        "encoder_inputs = training_model.input[0]\n",
        "encoder_outputs, state_h_enc, state_c_enc = training_model.layers[2].output\n",
        "encoder_states = [state_h_enc, state_c_enc]\n",
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "latent_dim = 256\n",
        "decoder_state_input_hidden = Input(shape=(latent_dim,))\n",
        "decoder_state_input_cell = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_hidden, decoder_state_input_cell]\n",
        "decoder_outputs, state_hidden, state_cell = lstm(decoder_input, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_hidden, state_cell]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_input] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnIJ0FZtgBLI"
      },
      "source": [
        "# model for testing \n",
        "def decode_response(test_input):\n",
        "    #Getting the output states to pass into the decoder\n",
        "    states_value = encoder_model.predict(test_input)\n",
        "    #Generating empty target sequence of length 1\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    #Setting the first token of target sequence with the start token\n",
        "    target_seq[0, 0, target_features_dict['<START>']] = 1.\n",
        "    \n",
        "    #A variable to store our response word by word\n",
        "    decoded_sentence = ''\n",
        "    \n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "      #Predicting output tokens with probabilities and states\n",
        "      output_tokens, hidden_state, cell_state = decoder_model.predict([target_seq] + states_value)\n",
        "      #Choosing the one with highest probability\n",
        "      sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "      sampled_token = reverse_target_features_dict[sampled_token_index]\n",
        "      decoded_sentence += \" \" + sampled_token#Stop if hit max length or found the stop token\n",
        "      if (sampled_token == '<END>' or len(decoded_sentence) > max_decoder_seq_length):\n",
        "        stop_condition = True\n",
        "      #Update the target sequence\n",
        "      target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "      target_seq[0, 0, sampled_token_index] = 1.\n",
        "      #Update states\n",
        "      states_value = [hidden_state, cell_state]\n",
        "    return decoded_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD-PeyKrJxg3"
      },
      "source": [
        "class Translator:\n",
        "  exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
        "  \n",
        "  #Method to start the translator\n",
        "  def start(self):\n",
        "    user_response = input(\"Give in an English sentence. :) \\n\")\n",
        "    self.translate(user_response)\n",
        "  \n",
        "  #Method to handle the conversation\n",
        "  def translate(self, reply):\n",
        "    while not self.make_exit(reply):\n",
        "      reply = input(self.generate_response(reply)+\"\\n\")#Method to convert user input into a matrix\n",
        "  def string_to_matrix(self, user_input):\n",
        "    tokens = re.findall(r\"[\\w']+|[^\\s\\w]\", user_input)\n",
        "    user_input_matrix = np.zeros(\n",
        "      (1, max_encoder_seq_length, num_encoder_tokens),\n",
        "      dtype='float32')\n",
        "    for timestep, token in enumerate(tokens):\n",
        "      if token in input_features_dict:\n",
        "        user_input_matrix[0, timestep, input_features_dict[token]] = 1.\n",
        "    return user_input_matrix\n",
        "  \n",
        "  #Method that will create a response using seq2seq model we built\n",
        "  def generate_response(self, user_input):\n",
        "    input_matrix = self.string_to_matrix(user_input)\n",
        "    chatbot_response = decode_response(input_matrix)\n",
        "    #Remove <START> and <END> tokens from chatbot_response\n",
        "    chatbot_response = chatbot_response.replace(\"<START>\",'')\n",
        "    chatbot_response = chatbot_response.replace(\"<END>\",'')\n",
        "    return chatbot_response\n",
        "  \n",
        "  #Method to check for exit commands\n",
        "  def make_exit(self, reply):\n",
        "    for exit_command in self.exit_commands:\n",
        "      if exit_command in reply:\n",
        "        print(\"Ok, have a great day!\")\n",
        "        return True\n",
        "    return False\n",
        "  \n",
        "translator = Translator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeYTg7GOgEwK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}